{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[INFO] Loaded EAST text detector 0.944140 seconds ...\n"
    }
   ],
   "source": [
    "# not working, but help to understand\n",
    "from pytextractor import pytextractor\n",
    "\n",
    "extractor = pytextractor.PyTextractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not use now, but could help in future\n",
    "import numpy as np\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "\n",
    "def get_rows(X):\n",
    "    # #############################################################################\n",
    "    # Generate sample data\n",
    "    #X = [1,2,4,7,9,5,4,7,9,56,57,54,60,200,297,275,243]\n",
    "    X = np.reshape(X, (-1, 1))\n",
    "\n",
    "    # #############################################################################\n",
    "    # Compute clustering with MeanShift\n",
    "\n",
    "    # The following bandwidth can be automatically detected using\n",
    "    # bandwidth = estimate_bandwidth(X, quantile=0.2, n_samples=100)\n",
    "\n",
    "    ms = MeanShift(bandwidth=None, bin_seeding=True)\n",
    "    ms.fit(X)\n",
    "    labels = ms.labels_\n",
    "    cluster_centers = ms.cluster_centers_\n",
    "\n",
    "    labels_unique = np.unique(labels)\n",
    "    n_clusters_ = len(labels_unique)\n",
    "\n",
    "    print(\"number of estimated clusters : %d\" % n_clusters_)\n",
    "    print(labels)\n",
    "    print(X)\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "\n",
    "def sort_contours(cnts):\n",
    "\tboundingBoxes = [cv2.boundingRect(c) for c in cnts]\n",
    "    # method= left-to-right and then top-to-bottom\n",
    "\t(cnts, boundingBoxes) = zip(* sorted ( sorted(zip(cnts, boundingBoxes),key=lambda b:b[1][0]), key=lambda b:b[1][1] ))\n",
    "\t# return the list of sorted contours and bounding boxes\n",
    "\treturn (cnts, boundingBoxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "\n",
    "extracted_text = []\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Users\\Aaron\\AppData\\Local\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def box_extraction(img_for_box_extraction_path, cropped_dir_path):\n",
    "    img = cv2.imread(img_for_box_extraction_path, 0)  # Read the image\n",
    "    (thresh, img_bin) = cv2.threshold(img, 127, 255, cv2.THRESH_TOZERO)  # Thresholding the image\n",
    "    img_bin = 255-img_bin  # Invert the image\n",
    "    cv2.imwrite(\"Image_bin.jpg\",img_bin)\n",
    "   \n",
    "    # Defining a kernel length\n",
    "    kernel_length = np.array(img).shape[1]//40\n",
    "     \n",
    "    # A verticle kernel of (1 X kernel_length), which will detect all the verticle lines from the image.\n",
    "    verticle_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (1, kernel_length))\n",
    "    # A horizontal kernel of (kernel_length X 1), which will help to detect all the horizontal line from the image.\n",
    "    hori_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (kernel_length, 1))\n",
    "    # A kernel of (3 X 3) ones.\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3))\n",
    "# Morphological operation to detect verticle lines from an image\n",
    "    img_temp1 = cv2.erode(img_bin, verticle_kernel, iterations=3)\n",
    "    verticle_lines_img = cv2.dilate(img_temp1, verticle_kernel, iterations=3)\n",
    "    cv2.imwrite(\"verticle_lines.jpg\",verticle_lines_img)\n",
    "# Morphological operation to detect horizontal lines from an image\n",
    "    img_temp2 = cv2.erode(img_bin, hori_kernel, iterations=3)\n",
    "    horizontal_lines_img = cv2.dilate(img_temp2, hori_kernel, iterations=3)\n",
    "    cv2.imwrite(\"horizontal_lines.jpg\",horizontal_lines_img)\n",
    "# Weighting parameters, this will decide the quantity of an image to be added to make a new image.\n",
    "    alpha = 0.5\n",
    "    beta = 1.0 - alpha\n",
    "    # This function helps to add two image with specific weight parameter to get a third image as summation of two image.\n",
    "    img_final_bin = cv2.addWeighted(verticle_lines_img, alpha, horizontal_lines_img, beta, 0.0)\n",
    "    img_final_bin = cv2.erode(~img_final_bin, kernel, iterations=2)\n",
    "    (thresh, img_final_bin) = cv2.threshold(img_final_bin, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "# For Debugging\n",
    "    # Enable this line to see verticle and horizontal lines in the image which is used to find boxes\n",
    "    cv2.imwrite(\"img_final_bin.jpg\",img_final_bin)\n",
    "    # Find contours for image, which will detect all the boxes\n",
    "    contours, hierarchy = cv2.findContours(img_final_bin, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Sort all the contours by top to bottom.\n",
    "    (contours, boundingBoxes) = sort_contours(contours)\n",
    "    idx = 0\n",
    "   \n",
    "    #X = [ cv2.boundingRect(c)[1] for c in contours ]\n",
    "    #print([ cv2.boundingRect(c)  for c in contours ])\n",
    "    #row_label = get_rows(X)\n",
    "    #cv2.drawContours(img, contours, -1, (0,255,0), 3)\n",
    "    #cv2.imwrite(\"img_final_see.jpg\",img)\n",
    "    row = -1\n",
    "\n",
    "    for c in contours:\n",
    "        # Returns the location and width,height for every contour\n",
    "        x, y, w, h = cv2.boundingRect(c)\n",
    "        \n",
    "# If the box height is greater then 20, widht is >80, then only save it as a box in \"cropped/\" folder.\n",
    "        #if (w > 80 and h > 20) and w > 3*h:\n",
    "        idx += 1\n",
    "     \n",
    "        new_img = img[y:y+h, x:x+w]\n",
    "\n",
    "        (thresh, new_img) = cv2.threshold(new_img, 128, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "\n",
    "        new_img_path = cropped_dir_path+str(idx) + '.png'\n",
    "        cv2.imwrite(new_img_path, new_img)\n",
    "        extracted_text.append(pytesseract.image_to_string(  Image.open(new_img_path), config ='--psm 6' ))\n",
    "\n",
    "        #extracted_text.append(pytesseract.image_to_string(  Image.fromarray(new_img) ))\n",
    "\n",
    "        if row != y:\n",
    "            row = y\n",
    "            extracted_text.append('\\n') \n",
    "        else:\n",
    "            extracted_text.append(',') \n",
    "       \n",
    "        \n",
    "\n",
    "box_extraction(\"Capture.JPG\", \"./Cropped/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "PageNum\n"
    }
   ],
   "source": [
    "import pytesseract\n",
    "\n",
    "text = pytesseract.image_to_string(Image.open('cropped/4.png'))\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "number of estimated clusters : 2\n[0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}